# -*- coding: utf-8 -*-
"""Bank_Note_DS_Module.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r0SRvK0pEx4BrEXOiOaGeHz1RBBJdLxx
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# !pip install sklearn

# !wget https://raw.githubusercontent.com/bamartin1618/DS_CC_Dataset/main/bank_note_data.csv

bank = pd.read_csv('bank_note_data.csv')

print(bank.size)
bank.dropna(axis=0, inplace =True)
bank.describe().T

"""# PLOTS AND UNDERSTANDING THE DATA"""

columns = ['Variance', 'Skew', 'Curt', 'Entropy']

# iterate through each collumns 0-3 to create 
fig, axs = plt.subplots(4, 4)

for i in [0, 1, 2, 3]:
  x_label = columns[i]
  for j in [0, 1, 2, 3]:
    y_label = columns[j]
    axs[i, j].scatter(bank.iloc[:, i], bank.iloc[:, j], s = .2)
    axs[j, i].set(xlabel=x_label)
    axs[i, 0].set(ylabel=x_label)
plt.subplots_adjust(left=1,
                    bottom=2,
                    right=3,
                    top=3,
                    wspace=.2,
                    hspace=0)
plt.suptitle("Scatterplots: Feature by Feature", x=2, y=3.07)
plt.show()

from plotly.subplots import make_subplots
import plotly.graph_objects as go
import math

fig = make_subplots(rows=4, cols=4)
col = bank.columns

for i in [0, 1, 2, 3]:
  x_label = col[i]
  for j in [0, 1, 2, 3]:
    y_label = col[j]
    fig.add_trace(
      go.Scatter(x=bank[x_label], y=bank[y_label], mode="markers", name=x_label),
      row=i+1, col=j+1
    )


fig.update_layout(height=600, width=800, title_text="Scatterplot: Features in relation to GDP")
fig.show()

columns = bank.columns
label = ["Variance", "Skew", "Curt", "Entropy"]
# iterate through each collumns 0-3 to create 

for i in range(0, 4):
  x_label = columns[i]
  for j in range(0, 4):
    y_label = columns[j]
    sns.scatterplot(data=bank, x=columns[i], y=columns[j], hue='Class',legend='auto')
    plt.title(f"Scatterplot of {label[j]} vs. {label[i]}")
    plt.show()

import plotly.express as px
columns = bank.columns
data = bank.iloc[:, :4]
#
# Correlation between different variables
#
corr = data.corr()
#
# Set up the matplotlib plot configuration
#
f, ax = plt.subplots(figsize=(6, 6))
#
# Generate a mask for upper traingle
#
mask = np.triu(np.ones_like(corr, dtype=bool))
#
# Configure a custom diverging colormap
#
cmap = sns.diverging_palette(230, 20, as_cmap=True)
#
# Draw the heatmap
#
sns.heatmap(corr, annot=True, mask = mask, cmap=cmap)

"""# TRAINING MODELS

We will be using XGBoost to train this dataset
"""

from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier

train_df, test_df = train_test_split(bank, train_size=.90, shuffle=True);

train_features, train_class = train_df.iloc[:, :4], train_df.iloc[:, 4]
test_features, test_class = test_df.iloc[:, :4], test_df.iloc[:, 4]

XGB = GradientBoostingClassifier(n_estimators=100, learning_rate=1, max_depth=1, random_state=0).fit(train_features, train_class)
predicted_XGB = XGB.predict(test_features)

report_XGB = classification_report(test_class.to_numpy(), predicted_XGB)
print(report_XGB)
confusion_XGB = confusion_matrix(test_class.to_numpy(), predicted_XGB)
print(confusion_XGB)

disp = ConfusionMatrixDisplay(confusion_XGB, display_labels=XGB.classes_)
disp.plot()

MLP = MLPClassifier(max_iter=300, early_stopping=False).fit(train_features, train_class)
predicted_MLP = MLP.predict(test_features)

report_MLP = classification_report(test_class.to_numpy(), predicted_MLP)
print(report_MLP)
confusion_MLP = confusion_matrix(test_class.to_numpy(), predicted_MLP)
print(confusion_MLP)

disp = ConfusionMatrixDisplay(confusion_MLP, display_labels=MLP.classes_)
disp.plot()

